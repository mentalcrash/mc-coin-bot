---
title: Backtesting Best Practices Guide
type: how-to
last_updated: 2026-01-28
status: draft
tags: [backtesting, validation, pitfalls, best-practices, quant]
---

# ğŸ¯ ë°±í…ŒìŠ¤íŒ… ëª¨ë²”ì‚¬ë¡€ ê°€ì´ë“œ

> [!WARNING]
> **"ë°±í…ŒìŠ¤íŠ¸ì—ì„œ ì„±ê³µí•œ ì „ëµì˜ ëŒ€ë¶€ë¶„ì€ ì‹¤ì „ì—ì„œ ì‹¤íŒ¨í•©ë‹ˆë‹¤."**
> 
> Duke University ì—°êµ¬ì— ë”°ë¥´ë©´, ë°±í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜(Look-ahead bias, Overfitting ë“±)ê°€
> ë¼ì´ë¸Œ íŠ¸ë ˆì´ë”© ì‹¤íŒ¨ì˜ ì£¼ìš” ì›ì¸ì…ë‹ˆë‹¤. ì´ ë¬¸ì„œì˜ ê°€ì´ë“œë¼ì¸ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”.

---

## ğŸ“‹ ëª©ì°¨

1. [í•µì‹¬ í•¨ì •ê³¼ í•´ê²°ì±…](#1-í•µì‹¬-í•¨ì •ê³¼-í•´ê²°ì±…)
2. [ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬](#2-ë°ì´í„°-í’ˆì§ˆ-ê´€ë¦¬)
3. [í˜„ì‹¤ì  ë¹„ìš© ëª¨ë¸ë§](#3-í˜„ì‹¤ì -ë¹„ìš©-ëª¨ë¸ë§)
4. [ê²€ì¦ ë°©ë²•ë¡ ](#4-ê²€ì¦-ë°©ë²•ë¡ )
5. [êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸](#5-êµ¬í˜„-ì²´í¬ë¦¬ìŠ¤íŠ¸)
6. [ì½”ë“œ íŒ¨í„´ ê°€ì´ë“œ](#6-ì½”ë“œ-íŒ¨í„´-ê°€ì´ë“œ)

---

## 1. í•µì‹¬ í•¨ì •ê³¼ í•´ê²°ì±…

### 1.1 Look-Ahead Bias (ë¯¸ë˜ ì •ë³´ ì°¸ì¡° ì˜¤ë¥˜)

> **ì •ì˜:** ì‹¤ì œ íŠ¸ë ˆì´ë”© ì‹œì ì—ì„œëŠ” ì•Œ ìˆ˜ ì—†ì—ˆë˜ ë¯¸ë˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì˜¤ë¥˜

#### ğŸ”´ ë°œìƒ ì›ì¸

```mermaid
flowchart LR
    subgraph "Look-Ahead Bias ì˜ˆì‹œ"
        A[í˜„ì¬ ì‹œì  t] --> B[ë¯¸ë˜ ì¢…ê°€ t+1 ì°¸ì¡°]
        B --> C[ë§¤ë§¤ ê²°ì •]
        C --> D[ë¹„í˜„ì‹¤ì  ìˆ˜ìµ]
    end
```

| ìœ í˜• | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| **ì½”ë“œ ì˜¤ë¥˜** | ì¸ë±ì‹± ì‹¤ìˆ˜ë¡œ ë¯¸ë˜ ë°ì´í„° ì°¸ì¡° | `df['close'].shift(-1)` ì‚¬ìš© |
| **ë°ì´í„° íƒ€ì´ë°** | ì§€í‘œ ê³„ì‚° ì‹œì  ì˜¤ë¥˜ | RSIë¥¼ ì¢…ê°€ í™•ì • ì „ ê³„ì‚° |
| **ì •ë³´ ì§€ì—° ë¬´ì‹œ** | ê³µì‹œ/ë‰´ìŠ¤ ë°˜ì˜ ì‹œê°„ ë¬´ì‹œ | ë°œí‘œ ì¦‰ì‹œ ë§¤ë§¤ ê°€ì • |

#### âœ… í•´ê²°ì±…

```python
# âŒ BAD: ë¯¸ë˜ ë°ì´í„° ì°¸ì¡°
def bad_signal(df: pd.DataFrame) -> pd.Series:
    # shift(-1)ì€ ë¯¸ë˜ ë°ì´í„°ë¥¼ í˜„ì¬ë¡œ ë‹¹ê¹€
    future_return = df["close"].shift(-1) / df["close"] - 1
    return future_return > 0.01  # ë¯¸ë˜ ìˆ˜ìµë¥ ë¡œ í˜„ì¬ ê²°ì •

# âœ… GOOD: ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©
def good_signal(df: pd.DataFrame) -> pd.Series:
    # shift(1)ì€ ê³¼ê±° ë°ì´í„°ë¥¼ í˜„ì¬ë¡œ ê°€ì ¸ì˜´
    past_return = df["close"] / df["close"].shift(1) - 1
    momentum = past_return.rolling(24).mean()  # ê³¼ê±° 24ì‹œê°„ í‰ê· 
    return momentum > 0.001
```

**í•„ìˆ˜ ê²€ì¦:**
```python
def validate_no_lookahead(signal_series: pd.Series, price_series: pd.Series) -> bool:
    """ì‹œê·¸ë„ì´ ë¯¸ë˜ ê°€ê²©ê³¼ ìƒê´€ê´€ê³„ê°€ ìˆìœ¼ë©´ Look-ahead bias ì˜ì‹¬."""
    future_returns = price_series.shift(-1) / price_series - 1
    correlation = signal_series.corr(future_returns)
    
    # ìƒê´€ê³„ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ìœ¼ë©´ ì˜ì‹¬
    if abs(correlation) > 0.3:
        raise ValueError(f"Potential look-ahead bias detected: corr={correlation:.3f}")
    return True
```

---

### 1.2 Survivorship Bias (ìƒì¡´ì í¸í–¥)

> **ì •ì˜:** í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì¢…ëª©ë§Œìœ¼ë¡œ ë°±í…ŒìŠ¤íŠ¸í•˜ì—¬ ìƒí/ì‹¤íŒ¨ ì¢…ëª©ì„ ì œì™¸í•˜ëŠ” ì˜¤ë¥˜

#### ğŸ”´ ë¬¸ì œì 

```mermaid
pie title 2024ë…„ ì•”í˜¸í™”í í˜„í™©
    "í˜„ì¬ ê±°ë˜ ê°€ëŠ¥" : 65
    "ìƒì¥íì§€/ì‚¬ë§" : 35
```

| ì˜í–¥ | ì„¤ëª… |
|------|------|
| **ìˆ˜ìµë¥  ê³¼ëŒ€í‰ê°€** | ì‹¤íŒ¨í•œ ì½”ì¸ì˜ ì†ì‹¤ì´ ì œì™¸ë¨ |
| **ìœ„í—˜ ê³¼ì†Œí‰ê°€** | íŒŒì‚°/ìƒí ìœ„í—˜ì´ ë°˜ì˜ ì•ˆ ë¨ |
| **ì „ëµ ì™œê³¡** | ì‹¤ì œë¡œëŠ” ìƒí ì½”ì¸ì— íˆ¬ìí–ˆì„ ìˆ˜ ìˆìŒ |

#### âœ… í•´ê²°ì±…

```python
# 1. ìƒì¡´ì í¸í–¥ ì—†ëŠ” ìœ ë‹ˆë²„ìŠ¤ êµ¬ì„±
def get_historical_universe(date: datetime) -> list[str]:
    """í•´ë‹¹ ì‹œì ì— ê±°ë˜ ê°€ëŠ¥í–ˆë˜ ëª¨ë“  ì¢…ëª© ë°˜í™˜ (ìƒí í¬í•¨)."""
    query = """
        SELECT DISTINCT symbol 
        FROM historical_listings
        WHERE listing_date <= :date
        AND (delisting_date IS NULL OR delisting_date > :date)
    """
    return db.execute(query, {"date": date}).fetchall()

# 2. ìƒí ì¢…ëª© ì²˜ë¦¬
def handle_delisting(position: Position, delisting_price: Decimal) -> Trade:
    """ìƒí ì‹œ ê°•ì œ ì²­ì‚° ì²˜ë¦¬."""
    return Trade(
        symbol=position.symbol,
        side="SELL",
        quantity=position.quantity,
        price=delisting_price * Decimal("0.5"),  # ìƒí ì‹œ 50% ì†ì‹¤ ê°€ì •
        reason="DELISTING",
    )
```

**ë°ì´í„° ìš”êµ¬ì‚¬í•­:**
- [ ] ìƒì¥/ìƒí ì´ë ¥ ë°ì´í„° í™•ë³´
- [ ] ìƒí ì¢…ëª©ì˜ ë§ˆì§€ë§‰ ê°€ê²© ë°ì´í„° ë³´ì¡´
- [ ] ìœ ì˜ì¢…ëª© ì§€ì • ì´ë ¥ ë°ì´í„°

---

### 1.3 Overfitting (ê³¼ì í•©)

> **ì •ì˜:** ê³¼ê±° ë°ì´í„°ì—ë§Œ ìµœì í™”ë˜ì–´ ë¯¸ë˜ì—ëŠ” ì‘ë™í•˜ì§€ ì•ŠëŠ” ì „ëµ

#### ğŸ”´ ê³¼ì í•© ì§•í›„

| ì§•í›„ | ì„¤ëª… |
|------|------|
| **íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ìŒ** | ë°ì´í„° í¬ì¸íŠ¸ ëŒ€ë¹„ íŒŒë¼ë¯¸í„° ê³¼ë‹¤ |
| **In-Sample ì„±ê³¼ë§Œ ì¢‹ìŒ** | Out-of-Sampleì—ì„œ ê¸‰ê²©íˆ í•˜ë½ |
| **íŠ¹ì • ê¸°ê°„ì—ë§Œ ì‘ë™** | 2024ë…„ì—ë§Œ ì¢‹ê³  2023ë…„ì€ ë‚˜ì¨ |
| **ë…¼ë¦¬ì  ì„¤ëª… ë¶ˆê°€** | ì™œ ì‘ë™í•˜ëŠ”ì§€ ì„¤ëª… ëª»í•¨ |

#### ğŸ“Š ê³¼ì í•© íƒì§€ ì§€í‘œ

```python
def calculate_overfitting_probability(
    in_sample_sharpe: float,
    out_sample_sharpe: float,
    num_parameters: int,
    num_trials: int,
) -> float:
    """
    ê³¼ì í•© í™•ë¥  ì¶”ì • (Probability of Backtest Overfitting, PBO).
    
    Bailey et al. (2014) ë°©ë²•ë¡  ê¸°ë°˜.
    """
    # ì„±ê³¼ í•˜ë½ë¥ 
    performance_degradation = 1 - (out_sample_sharpe / in_sample_sharpe)
    
    # íŒŒë¼ë¯¸í„° í˜ë„í‹°
    param_penalty = num_parameters / num_trials
    
    # ê³¼ì í•© í™•ë¥  (ë‹¨ìˆœí™”ëœ íœ´ë¦¬ìŠ¤í‹±)
    pbo = min(1.0, performance_degradation + param_penalty)
    
    return pbo
```

#### âœ… ê³¼ì í•© ë°©ì§€ ì „ëµ

```mermaid
flowchart TD
    A[ì „ëµ ê°œë°œ] --> B{íŒŒë¼ë¯¸í„° ìˆ˜}
    B -->|ë§ìŒ > 5| C[ì°¨ì› ì¶•ì†Œ]
    B -->|ì ì • <= 5| D[Cross Validation]
    
    D --> E[Walk-Forward Test]
    E --> F{Out-Sample ì„±ê³¼}
    F -->|In-Sampleì˜ 70% ì´ìƒ| G[âœ… ì±„íƒ]
    F -->|70% ë¯¸ë§Œ| H[âŒ ê³¼ì í•© ì˜ì‹¬]
    
    H --> I[íŒŒë¼ë¯¸í„° ë‹¨ìˆœí™”]
    I --> A
```

**ê¶Œì¥ íŒŒë¼ë¯¸í„° ìˆ˜:**
```
ìµœëŒ€ íŒŒë¼ë¯¸í„° ìˆ˜ = sqrt(ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜) / 10

ì˜ˆì‹œ: 2ë…„ 1ì‹œê°„ë´‰ = 17,520ê°œ ë°ì´í„°
     sqrt(17,520) / 10 â‰ˆ 13ê°œ íŒŒë¼ë¯¸í„° ì´í•˜
```

---

### 1.4 Data Snooping (ë°ì´í„° ì—¼íƒ)

> **ì •ì˜:** ë™ì¼í•œ ë°ì´í„°ë¡œ ì—¬ëŸ¬ ì „ëµì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ìš°ì—°íˆ ì¢‹ì€ ê²°ê³¼ë¥¼ ì°¾ëŠ” ì˜¤ë¥˜

#### ğŸ”´ ë¬¸ì œì 

```
100ê°œ ì „ëµ í…ŒìŠ¤íŠ¸ â†’ 5ê°œê°€ ìœ ì˜ë¯¸ (p < 0.05)
â†’ ì‹¤ì œë¡œëŠ” ìš°ì—°ì˜ ê²°ê³¼ (ë‹¤ì¤‘ ê²€ì • ë¬¸ì œ)
```

#### âœ… í•´ê²°ì±…: Bonferroni ë³´ì •

```python
def apply_multiple_testing_correction(
    p_values: list[float],
    num_tests: int,
    method: str = "bonferroni",
) -> list[bool]:
    """ë‹¤ì¤‘ ê²€ì • ë³´ì •."""
    if method == "bonferroni":
        # Bonferroni: ìœ ì˜ìˆ˜ì¤€ì„ í…ŒìŠ¤íŠ¸ íšŸìˆ˜ë¡œ ë‚˜ëˆ”
        adjusted_alpha = 0.05 / num_tests
        return [p < adjusted_alpha for p in p_values]
    elif method == "holm":
        # Holm-Bonferroni: ë‹¨ê³„ì  ë³´ì •
        sorted_indices = sorted(range(len(p_values)), key=lambda i: p_values[i])
        significant = [False] * len(p_values)
        for rank, idx in enumerate(sorted_indices):
            adjusted_alpha = 0.05 / (num_tests - rank)
            if p_values[idx] < adjusted_alpha:
                significant[idx] = True
            else:
                break
        return significant
    else:
        raise ValueError(f"Unknown method: {method}")
```

**í•„ìˆ˜ ê¸°ë¡:**
- [ ] í…ŒìŠ¤íŠ¸í•œ ëª¨ë“  ì „ëµ ë²„ì „ ê¸°ë¡
- [ ] ê° ì „ëµì˜ p-value ë° íš¨ê³¼ í¬ê¸° ê¸°ë¡
- [ ] ìµœì¢… ì„ íƒ ì „ëµì˜ ì„ íƒ ì´ìœ  ë¬¸ì„œí™”

---

## 2. ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬

### 2.1 ë°ì´í„° ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

| í•­ëª© | ê²€ì¦ ë°©ë²• | ì„ê³„ê°’ |
|------|----------|--------|
| **ê²°ì¸¡ì¹˜** | `df.isnull().sum()` | < 0.1% |
| **ì¤‘ë³µ íƒ€ì„ìŠ¤íƒ¬í”„** | `df.index.duplicated()` | 0ê°œ |
| **ì‹œê°„ ì—°ì†ì„±** | ê°„ê²© ì¼ì •ì„± í™•ì¸ | ëª¨ë“  ê°„ê²© ë™ì¼ |
| **ê°€ê²© ì´ìƒì¹˜** | Z-score > 5 íƒì§€ | ìˆ˜ë™ ê²€í†  |
| **OHLC ì¼ê´€ì„±** | High >= Low, High >= Open/Close | 100% í†µê³¼ |

### 2.2 ë°ì´í„° ê²€ì¦ ì½”ë“œ

```python
from dataclasses import dataclass
from loguru import logger

@dataclass
class DataQualityReport:
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼."""
    missing_ratio: float
    duplicate_count: int
    gap_count: int
    outlier_count: int
    ohlc_violations: int
    is_valid: bool


def validate_ohlcv_data(df: pd.DataFrame) -> DataQualityReport:
    """OHLCV ë°ì´í„° í’ˆì§ˆ ê²€ì¦."""
    # 1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨
    missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
    
    # 2. ì¤‘ë³µ íƒ€ì„ìŠ¤íƒ¬í”„
    duplicate_count = df.index.duplicated().sum()
    
    # 3. ì‹œê°„ ê°„ê²© ê²€ì‚¬
    time_diffs = df.index.to_series().diff()
    expected_gap = time_diffs.mode()[0]
    gap_count = (time_diffs != expected_gap).sum() - 1  # ì²« ë²ˆì§¸ NaN ì œì™¸
    
    # 4. ì´ìƒì¹˜ íƒì§€ (ì¢…ê°€ ê¸°ì¤€ Z-score)
    z_scores = (df["close"] - df["close"].mean()) / df["close"].std()
    outlier_count = (abs(z_scores) > 5).sum()
    
    # 5. OHLC ì¼ê´€ì„±
    ohlc_violations = (
        (df["high"] < df["low"]).sum() +
        (df["high"] < df["open"]).sum() +
        (df["high"] < df["close"]).sum() +
        (df["low"] > df["open"]).sum() +
        (df["low"] > df["close"]).sum()
    )
    
    # ì¢…í•© íŒì •
    is_valid = (
        missing_ratio < 0.001 and
        duplicate_count == 0 and
        gap_count < len(df) * 0.01 and
        ohlc_violations == 0
    )
    
    report = DataQualityReport(
        missing_ratio=missing_ratio,
        duplicate_count=duplicate_count,
        gap_count=gap_count,
        outlier_count=outlier_count,
        ohlc_violations=ohlc_violations,
        is_valid=is_valid,
    )
    
    if not is_valid:
        logger.warning(f"Data quality check failed: {report}")
    
    return report
```

### 2.3 ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ

| ê²°ì¸¡ì¹˜ ìœ í˜• | ì²˜ë¦¬ ë°©ë²• | ì½”ë“œ |
|------------|----------|------|
| **ë‹¨ì¼ ìº”ë“¤** | Forward Fill | `df.ffill()` |
| **ì—°ì† < 5ê°œ** | ì„ í˜• ë³´ê°„ | `df.interpolate()` |
| **ì—°ì† >= 5ê°œ** | í•´ë‹¹ êµ¬ê°„ ì œì™¸ | ë³„ë„ ì²˜ë¦¬ |

```python
def handle_missing_data(df: pd.DataFrame, max_consecutive: int = 5) -> pd.DataFrame:
    """ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì—°ì† ê²°ì¸¡ í•œë„ ì ìš©)."""
    # ì—°ì† ê²°ì¸¡ êµ¬ê°„ íƒì§€
    is_null = df["close"].isnull()
    consecutive_nulls = is_null.groupby((~is_null).cumsum()).cumsum()
    
    # í•œë„ ì´ˆê³¼ êµ¬ê°„ ë§ˆí‚¹
    excessive_gaps = consecutive_nulls > max_consecutive
    
    if excessive_gaps.any():
        logger.warning(f"Found {excessive_gaps.sum()} points with excessive gaps")
        # í•´ë‹¹ êµ¬ê°„ ì‹œì‘/ë ë¡œê¹…
        gap_starts = excessive_gaps & ~excessive_gaps.shift(1, fill_value=False)
        for ts in df.index[gap_starts]:
            logger.warning(f"Gap starts at: {ts}")
    
    # í—ˆìš© ë²”ìœ„ ë‚´ ê²°ì¸¡ì€ ë³´ê°„
    df_filled = df.interpolate(method="linear", limit=max_consecutive)
    
    return df_filled
```

---

## 3. í˜„ì‹¤ì  ë¹„ìš© ëª¨ë¸ë§

### 3.1 ê±°ë˜ ë¹„ìš© êµ¬ì„± ìš”ì†Œ

```mermaid
flowchart LR
    subgraph "ëª…ì‹œì  ë¹„ìš©"
        A[ê±°ë˜ ìˆ˜ìˆ˜ë£Œ]
        B[í€ë”©ë¹„]
    end
    
    subgraph "ì•”ë¬µì  ë¹„ìš©"
        C[ìŠ¬ë¦¬í”¼ì§€]
        D[ë§ˆì¼“ ì„íŒ©íŠ¸]
    end
    
    A & B & C & D --> E[ì´ ê±°ë˜ ë¹„ìš©]
```

### 3.2 ë¹„ìš© íŒŒë¼ë¯¸í„°

| ë¹„ìš© í•­ëª© | ë³´ìˆ˜ì  ê°€ì • | ë‚™ê´€ì  ê°€ì • | í˜„ì‹¤ì  ê°€ì • |
|----------|------------|------------|------------|
| **Maker Fee** | 0.02% | 0.01% | 0.02% |
| **Taker Fee** | 0.05% | 0.03% | 0.04% |
| **Slippage** | 0.10% | 0.03% | 0.05% |
| **Market Impact** | 0.05% | 0.01% | 0.02% |
| **Funding (8h)** | 0.03% | 0.005% | 0.01% |

> [!TIP]
> **ê¶Œì¥:** ì´ˆê¸° ë°±í…ŒìŠ¤íŠ¸ëŠ” **ë³´ìˆ˜ì  ê°€ì •**ìœ¼ë¡œ ì‹œì‘í•˜ê³ ,
> ì‹¤ê±°ë˜ ë°ì´í„° ì¶•ì  í›„ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”.

### 3.3 ë¹„ìš© ëª¨ë¸ êµ¬í˜„

```python
from decimal import Decimal
from pydantic import BaseModel

class CostModelConfig(BaseModel):
    """ê±°ë˜ ë¹„ìš© ëª¨ë¸ ì„¤ì •."""
    maker_fee: Decimal = Decimal("0.0002")      # 0.02%
    taker_fee: Decimal = Decimal("0.0004")      # 0.04%
    slippage_rate: Decimal = Decimal("0.0005")  # 0.05%
    market_impact_rate: Decimal = Decimal("0.0002")  # 0.02%
    funding_rate_8h: Decimal = Decimal("0.0001")     # 0.01%


class CostModel:
    """í˜„ì‹¤ì  ê±°ë˜ ë¹„ìš© ê³„ì‚°."""
    
    def __init__(self, config: CostModelConfig) -> None:
        self._config = config
    
    def calculate_entry_cost(
        self,
        notional_value: Decimal,
        is_maker: bool = False,
    ) -> Decimal:
        """ì§„ì… ì‹œ ë¹„ìš© ê³„ì‚°."""
        fee = self._config.maker_fee if is_maker else self._config.taker_fee
        slippage = self._config.slippage_rate
        
        total_rate = fee + slippage
        return notional_value * total_rate
    
    def calculate_exit_cost(
        self,
        notional_value: Decimal,
        is_maker: bool = False,
    ) -> Decimal:
        """ì²­ì‚° ì‹œ ë¹„ìš© ê³„ì‚°."""
        # ì§„ì…ê³¼ ë™ì¼í•œ ë¡œì§
        return self.calculate_entry_cost(notional_value, is_maker)
    
    def calculate_holding_cost(
        self,
        notional_value: Decimal,
        holding_hours: int,
        is_long: bool = True,
    ) -> Decimal:
        """ë³´ìœ  ê¸°ê°„ ë¹„ìš© (í€ë”©ë¹„)."""
        funding_periods = holding_hours // 8
        
        # ë¡± í¬ì§€ì…˜: ì–‘ì˜ í€ë”©ë¹„ â†’ ë¹„ìš© ë°œìƒ
        # ìˆ í¬ì§€ì…˜: ì–‘ì˜ í€ë”©ë¹„ â†’ ìˆ˜ìµ ë°œìƒ (ìŒìˆ˜ ë¹„ìš©)
        direction = 1 if is_long else -1
        
        return notional_value * self._config.funding_rate_8h * funding_periods * direction
    
    def calculate_round_trip_cost(
        self,
        notional_value: Decimal,
        holding_hours: int = 0,
        is_long: bool = True,
    ) -> Decimal:
        """ì™•ë³µ ê±°ë˜ ì´ ë¹„ìš©."""
        entry_cost = self.calculate_entry_cost(notional_value)
        exit_cost = self.calculate_exit_cost(notional_value)
        holding_cost = self.calculate_holding_cost(notional_value, holding_hours, is_long)
        
        return entry_cost + exit_cost + holding_cost
```

### 3.4 ìŠ¬ë¦¬í”¼ì§€ ë™ì  ëª¨ë¸ë§

```python
def calculate_dynamic_slippage(
    order_size_usd: Decimal,
    avg_hourly_volume_usd: Decimal,
    volatility: float,
) -> Decimal:
    """
    ë™ì  ìŠ¬ë¦¬í”¼ì§€ ê³„ì‚°.
    
    ìŠ¬ë¦¬í”¼ì§€ = ê¸°ë³¸ ìŠ¬ë¦¬í”¼ì§€ Ã— (1 + ì£¼ë¬¸í¬ê¸°ë¹„ìœ¨) Ã— (1 + ë³€ë™ì„±ìŠ¹ìˆ˜)
    """
    BASE_SLIPPAGE = Decimal("0.0003")  # 0.03%
    
    # ì£¼ë¬¸ í¬ê¸° ë¹„ìœ¨ (ì‹œê°„ë‹¹ ê±°ë˜ëŸ‰ ëŒ€ë¹„)
    size_ratio = order_size_usd / avg_hourly_volume_usd
    size_multiplier = 1 + float(size_ratio)
    
    # ë³€ë™ì„± ìŠ¹ìˆ˜
    vol_multiplier = 1 + volatility * 10  # ë³€ë™ì„± 1%ë‹¹ 10% ì¶”ê°€
    
    dynamic_slippage = BASE_SLIPPAGE * Decimal(str(size_multiplier * vol_multiplier))
    
    return min(dynamic_slippage, Decimal("0.01"))  # ìµœëŒ€ 1% ìº¡
```

---

## 4. ê²€ì¦ ë°©ë²•ë¡ 

### 4.1 ë°ì´í„° ë¶„í•  ì „ëµ

```mermaid
gantt
    title ë°ì´í„° ë¶„í•  (2ë…„ ë°ì´í„° ê¸°ì¤€)
    dateFormat  YYYY-MM-DD
    section Training
    In-Sample (60%)     :is, 2024-01-01, 438d
    section Validation
    Validation (20%)    :val, after is, 146d
    section Test
    Out-of-Sample (20%) :oos, after val, 146d
```

| êµ¬ê°„ | ìš©ë„ | ë¹„ìœ¨ |
|------|------|------|
| **In-Sample** | ì „ëµ ê°œë°œ ë° íŒŒë¼ë¯¸í„° íŠœë‹ | 60% |
| **Validation** | íŒŒë¼ë¯¸í„° ì„ íƒ ê²€ì¦ | 20% |
| **Out-of-Sample** | ìµœì¢… ì„±ê³¼ í‰ê°€ (1íšŒë§Œ ì‚¬ìš©) | 20% |

### 4.2 Walk-Forward Optimization

> [!IMPORTANT]
> **Walk-ForwardëŠ” ê°€ì¥ ê°•ë ¥í•œ ê³¼ì í•© ë°©ì§€ ë°©ë²•ì…ë‹ˆë‹¤.**
> 
> ì‹¤ì œ ìš´ìš©ê³¼ ë™ì¼í•˜ê²Œ "ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµ â†’ ë¯¸ë˜ ë°ì´í„°ë¡œ ê²€ì¦"ì„ ë°˜ë³µí•©ë‹ˆë‹¤.

```mermaid
flowchart TB
    subgraph "Window 1"
        T1[Train: Jan-Jun 2024]
        V1[Test: Jul 2024]
    end
    
    subgraph "Window 2"
        T2[Train: Feb-Jul 2024]
        V2[Test: Aug 2024]
    end
    
    subgraph "Window 3"
        T3[Train: Mar-Aug 2024]
        V3[Test: Sep 2024]
    end
    
    T1 --> V1
    T2 --> V2
    T3 --> V3
    
    V1 & V2 & V3 --> AGG[Aggregate Results]
```

```python
from dataclasses import dataclass

@dataclass
class WalkForwardConfig:
    """Walk-Forward ì„¤ì •."""
    train_period_days: int = 180  # 6ê°œì›”
    test_period_days: int = 30   # 1ê°œì›”
    step_days: int = 30          # 1ê°œì›”ì”© ì´ë™


def walk_forward_optimization(
    data: pd.DataFrame,
    strategy_factory: Callable,
    param_grid: dict,
    config: WalkForwardConfig,
) -> list[dict]:
    """Walk-Forward Optimization ì‹¤í–‰."""
    results = []
    
    start_date = data.index.min()
    end_date = data.index.max()
    
    current_start = start_date
    
    while True:
        train_end = current_start + timedelta(days=config.train_period_days)
        test_end = train_end + timedelta(days=config.test_period_days)
        
        if test_end > end_date:
            break
        
        # 1. Training êµ¬ê°„ì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
        train_data = data[current_start:train_end]
        best_params = optimize_parameters(train_data, strategy_factory, param_grid)
        
        # 2. Test êµ¬ê°„ì—ì„œ ì„±ê³¼ ì¸¡ì •
        test_data = data[train_end:test_end]
        strategy = strategy_factory(**best_params)
        test_result = backtest(strategy, test_data)
        
        results.append({
            "train_start": current_start,
            "train_end": train_end,
            "test_start": train_end,
            "test_end": test_end,
            "best_params": best_params,
            "test_sharpe": test_result.sharpe_ratio,
            "test_return": test_result.total_return,
        })
        
        # ë‹¤ìŒ ìœˆë„ìš°ë¡œ ì´ë™
        current_start += timedelta(days=config.step_days)
    
    return results
```

### 4.3 í†µê³„ì  ìœ ì˜ì„± ê²€ì •

```python
import numpy as np
from scipy import stats

def calculate_statistical_significance(
    strategy_returns: np.ndarray,
    benchmark_returns: np.ndarray,
    confidence_level: float = 0.95,
) -> dict:
    """ì „ëµ ìˆ˜ìµë¥ ì˜ í†µê³„ì  ìœ ì˜ì„± ê²€ì •."""
    # 1. ì´ˆê³¼ ìˆ˜ìµë¥ 
    excess_returns = strategy_returns - benchmark_returns
    
    # 2. t-ê²€ì • (í‰ê·  ì´ˆê³¼ ìˆ˜ìµì´ 0ë³´ë‹¤ í°ì§€)
    t_stat, p_value = stats.ttest_1samp(excess_returns, 0)
    
    # 3. ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„
    n_bootstrap = 10000
    bootstrap_means = []
    for _ in range(n_bootstrap):
        sample = np.random.choice(excess_returns, size=len(excess_returns), replace=True)
        bootstrap_means.append(sample.mean())
    
    ci_lower = np.percentile(bootstrap_means, (1 - confidence_level) / 2 * 100)
    ci_upper = np.percentile(bootstrap_means, (1 + confidence_level) / 2 * 100)
    
    return {
        "mean_excess_return": excess_returns.mean(),
        "t_statistic": t_stat,
        "p_value": p_value,
        "is_significant": p_value < (1 - confidence_level),
        "confidence_interval": (ci_lower, ci_upper),
        "ci_excludes_zero": ci_lower > 0,  # ì‹ ë¢°êµ¬ê°„ì´ 0ì„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©´ ìœ ì˜ë¯¸
    }
```

### 4.4 Monte Carlo Simulation

```python
def monte_carlo_permutation_test(
    strategy_returns: np.ndarray,
    benchmark_returns: np.ndarray,
    n_simulations: int = 10000,
) -> float:
    """
    Monte Carlo ìˆœì—´ ê²€ì •.
    
    ì „ëµ ìˆ˜ìµë¥ ì´ ìš°ì—°ì´ ì•„ë‹˜ì„ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    observed_diff = strategy_returns.mean() - benchmark_returns.mean()
    
    combined = np.concatenate([strategy_returns, benchmark_returns])
    n_strategy = len(strategy_returns)
    
    count_greater = 0
    
    for _ in range(n_simulations):
        # ë¬´ì‘ìœ„ë¡œ ì„ì–´ì„œ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ”
        np.random.shuffle(combined)
        perm_strategy = combined[:n_strategy]
        perm_benchmark = combined[n_strategy:]
        
        perm_diff = perm_strategy.mean() - perm_benchmark.mean()
        
        if perm_diff >= observed_diff:
            count_greater += 1
    
    p_value = count_greater / n_simulations
    return p_value
```

---

## 5. êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 5.1 ë°±í…ŒìŠ¤íŠ¸ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ**
  - [ ] ê²°ì¸¡ì¹˜ < 0.1%
  - [ ] ì¤‘ë³µ íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ
  - [ ] OHLC ì¼ê´€ì„± í†µê³¼
  
- [ ] **Look-Ahead Bias ê²€ì¦**
  - [ ] ëª¨ë“  ì‹œê·¸ë„ ê³„ì‚°ì— `shift(1)` ì´ìƒ ì‚¬ìš©
  - [ ] ì§„ì…/ì²­ì‚° ê°€ê²©ì— ë¯¸ë˜ ë°ì´í„° ë¯¸ì‚¬ìš©
  
- [ ] **Survivorship Bias ê³ ë ¤**
  - [ ] ìƒí ì¢…ëª© ë°ì´í„° í¬í•¨ ì—¬ë¶€ í™•ì¸
  - [ ] ìƒí ì‹œ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„

### 5.2 ë°±í…ŒìŠ¤íŠ¸ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **í˜„ì‹¤ì  ë¹„ìš© ëª¨ë¸ ì ìš©**
  - [ ] ìˆ˜ìˆ˜ë£Œ (Maker/Taker)
  - [ ] ìŠ¬ë¦¬í”¼ì§€
  - [ ] í€ë”©ë¹„ (ì„ ë¬¼)
  
- [ ] **ì‹¤í–‰ í˜„ì‹¤ì„±**
  - [ ] ì£¼ë¬¸ ì‹¤í–‰ ì§€ì—° (1ë¶„ ì´ìƒ)
  - [ ] ë¶€ë¶„ ì²´ê²° ê°€ëŠ¥ì„±
  - [ ] ìœ ë™ì„± í•œë„

### 5.3 ë°±í…ŒìŠ¤íŠ¸ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ë²¤ì¹˜ë§ˆí¬ ë¹„êµ**
  - [ ] Buy & Hold ëŒ€ë¹„ ì„±ê³¼
  - [ ] Alpha / Beta ê³„ì‚°
  
- [ ] **í†µê³„ì  ê²€ì¦**
  - [ ] Sharpe Ratio ì‹ ë¢°êµ¬ê°„
  - [ ] p-value < 0.05
  - [ ] Walk-Forward ê²°ê³¼ ì¼ê´€ì„±
  
- [ ] **ê³¼ì í•© ê²€ì‚¬**
  - [ ] Out-of-Sample ì„±ê³¼ >= In-Sampleì˜ 70%
  - [ ] íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë¶„ì„ ì™„ë£Œ

### 5.4 ë¼ì´ë¸Œ ì „í™˜ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **Paper Trading ê²€ì¦**
  - [ ] ë°±í…ŒìŠ¤íŠ¸ì™€ Paper Trading ì„±ê³¼ ì¼ì¹˜ (Â±20%)
  - [ ] 7ì¼ ì´ìƒ Paper Trading ì™„ë£Œ
  
- [ ] **ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê²€ì¦**
  - [ ] Kill Switch ë™ì‘ í™•ì¸
  - [ ] Position Limit ë™ì‘ í™•ì¸

---

## 6. ì½”ë“œ íŒ¨í„´ ê°€ì´ë“œ

### 6.1 Anti-Patterns (í”¼í•´ì•¼ í•  íŒ¨í„´)

#### âŒ ë¯¸ë˜ ì°¸ì¡°

```python
# BAD: ë¯¸ë˜ ì¢…ê°€ë¡œ í˜„ì¬ ê²°ì •
df["signal"] = df["close"].shift(-1) > df["close"]
```

#### âŒ ë¹„í˜„ì‹¤ì  ì‹¤í–‰ ê°€ê²©

```python
# BAD: ì‹œê·¸ë„ ë°œìƒ ì‹œì ì˜ ì¢…ê°€ë¡œ ì¦‰ì‹œ ì²´ê²°
entry_price = df.loc[signal_time, "close"]
```

#### âŒ ë¹„ìš© ë¬´ì‹œ

```python
# BAD: ìˆ˜ìˆ˜ë£Œ/ìŠ¬ë¦¬í”¼ì§€ ì—†ì´ ìˆ˜ìµ ê³„ì‚°
profit = (exit_price - entry_price) * quantity
```

### 6.2 Best Patterns (ê¶Œì¥ íŒ¨í„´)

#### âœ… ì•ˆì „í•œ ì‹œê·¸ë„ ê³„ì‚°

```python
# GOOD: ê³¼ê±° ë°ì´í„°ë§Œ ì‚¬ìš©
df["signal"] = df["close"].shift(1) > df["close"].shift(2)
```

#### âœ… í˜„ì‹¤ì  ì‹¤í–‰ ê°€ê²©

```python
# GOOD: ë‹¤ìŒ ìº”ë“¤ ì‹œê°€ë¡œ ì²´ê²° ê°€ì •
def get_execution_price(
    signal_time: datetime,
    data: pd.DataFrame,
    slippage_rate: float = 0.0005,
) -> Decimal:
    """í˜„ì‹¤ì  ì²´ê²°ê°€ ê³„ì‚°."""
    # ì‹œê·¸ë„ ë‹¤ìŒ ìº”ë“¤ì˜ ì‹œê°€
    next_idx = data.index.get_loc(signal_time) + 1
    if next_idx >= len(data):
        return None
    
    open_price = data.iloc[next_idx]["open"]
    slippage = open_price * Decimal(str(slippage_rate))
    
    return open_price + slippage  # ë§¤ìˆ˜ ì‹œ ë¶ˆë¦¬í•˜ê²Œ
```

#### âœ… ì™„ì „í•œ ë¹„ìš© ëª¨ë¸

```python
# GOOD: ëª¨ë“  ë¹„ìš© í¬í•¨
def calculate_net_profit(
    entry_price: Decimal,
    exit_price: Decimal,
    quantity: Decimal,
    cost_model: CostModel,
    holding_hours: int,
    is_long: bool,
) -> Decimal:
    """ìˆœìˆ˜ìµ ê³„ì‚° (ëª¨ë“  ë¹„ìš© ì°¨ê°)."""
    gross_profit = (exit_price - entry_price) * quantity
    if not is_long:
        gross_profit = -gross_profit
    
    notional = entry_price * quantity
    total_cost = cost_model.calculate_round_trip_cost(
        notional, holding_hours, is_long
    )
    
    return gross_profit - total_cost
```

---

## ğŸ“ ë¶€ë¡

### A. ì°¸ê³  ìë£Œ

1. **Bailey et al. (2014)** - "Probability of Backtest Overfitting"
2. **Harvey et al. (2016)** - "...and the Cross-Section of Expected Returns"
3. **Duke University (2024)** - "5 Critical Backtesting Mistakes"
4. **Vontobel Asset Management** - "Backtesting Done Right"

### B. ìš©ì–´ì§‘

| ìš©ì–´ | ì •ì˜ |
|------|------|
| **Look-Ahead Bias** | ë¯¸ë˜ ì •ë³´ë¥¼ ê³¼ê±° ì˜ì‚¬ê²°ì •ì— ì‚¬ìš©í•˜ëŠ” ì˜¤ë¥˜ |
| **Survivorship Bias** | ì‹¤íŒ¨/ìƒí ì¢…ëª©ì„ ì œì™¸í•˜ì—¬ ì„±ê³¼ë¥¼ ê³¼ëŒ€í‰ê°€í•˜ëŠ” ì˜¤ë¥˜ |
| **Overfitting** | ê³¼ê±° ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ìµœì í™”ë˜ì–´ ë¯¸ë˜ ì˜ˆì¸¡ë ¥ì´ ì—†ëŠ” ìƒíƒœ |
| **Walk-Forward** | ì‹œê°„ìˆœìœ¼ë¡œ í•™ìŠµ/ê²€ì¦ì„ ë°˜ë³µí•˜ëŠ” ê²€ì¦ ë°©ë²• |
| **PBO** | Probability of Backtest Overfitting (ê³¼ì í•© í™•ë¥ ) |

### C. ë³€ê²½ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ë³€ê²½ ë‚´ìš© | ì‘ì„±ì |
|------|------|----------|--------|
| 0.1 | 2026-01-28 | ì´ˆì•ˆ ì‘ì„± | AI Assistant |

---

> [!CAUTION]
> ì´ ê°€ì´ë“œë¥¼ ë”°ë¥´ë”ë¼ë„ **ë¼ì´ë¸Œ íŠ¸ë ˆì´ë”©ì˜ ì„±ê³µì„ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
> 
> ë°±í…ŒìŠ¤íŠ¸ëŠ” ê³¼ê±° ë°ì´í„°ì— ê¸°ë°˜í•˜ë©°, ì‹œì¥ í™˜ê²½ì€ ì§€ì†ì ìœ¼ë¡œ ë³€í•©ë‹ˆë‹¤.
> í•­ìƒ ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ í•˜ê³ , ê°ë‹¹ ê°€ëŠ¥í•œ ê¸ˆì•¡ë§Œ íˆ¬ìí•˜ì„¸ìš”.
